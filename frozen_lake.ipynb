{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Value Iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating transition function: 100%|██████████| 1000/1000 [00:00<00:00, 1633.28it/s]\n",
      "Value Iteration: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
      "Testing policy: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts/20221118-084740/FrozenLake8x8-v1/value_iteration//value_iteration/policy.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 166\u001B[0m\n\u001B[0;32m    164\u001B[0m test_iterations \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m    165\u001B[0m test_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e3\u001B[39m\n\u001B[1;32m--> 166\u001B[0m \u001B[43mrun_value_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFrozenLake8x8-v1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iterations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_samples\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [12], line 146\u001B[0m, in \u001B[0;36mrun_value_iteration\u001B[1;34m(name, max_iter, test_iterations, test_samples)\u001B[0m\n\u001B[0;32m    143\u001B[0m P, success_rate \u001B[38;5;241m=\u001B[39m value_iteration(env, T, R, max_iter\u001B[38;5;241m=\u001B[39mmax_iter, test_iterations\u001B[38;5;241m=\u001B[39mtest_iterations)\n\u001B[0;32m    145\u001B[0m \u001B[38;5;66;03m# save all artifacts\u001B[39;00m\n\u001B[1;32m--> 146\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdir_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/value_iteration/policy.npy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mP\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdir_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/value_iteration/success_rate.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m, success_rate)\n\u001B[0;32m    148\u001B[0m np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdir_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/value_iteration/transition.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m, T)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36msave\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\numpy\\lib\\npyio.py:498\u001B[0m, in \u001B[0;36msave\u001B[1;34m(file, arr, allow_pickle, fix_imports)\u001B[0m\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m file\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    497\u001B[0m         file \u001B[38;5;241m=\u001B[39m file \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 498\u001B[0m     file_ctx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m file_ctx \u001B[38;5;28;01mas\u001B[39;00m fid:\n\u001B[0;32m    501\u001B[0m     arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(arr)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'artifacts/20221118-084740/FrozenLake8x8-v1/value_iteration//value_iteration/policy.npy'"
     ]
    }
   ],
   "source": [
    "## Value Iteration\n",
    "def get_transitional_func(env, samples=1e5):\n",
    "    num_states = 0\n",
    "    num_actions = 0\n",
    "    id = env.spec.id\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # transition probability\n",
    "    T = np.zeros((num_states, num_actions, num_states))\n",
    "    # reward function\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    counter_map = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "    counter = 0\n",
    "    for _ in tqdm(range(int(samples)), desc=\"Calculating transition function\"):\n",
    "        state = env.reset()\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            random_action = env.action_space.sample() # todo consider looping through all actions\n",
    "            observation, reward, terminated, info = env.step(random_action)\n",
    "            T[state][random_action][observation] += 1\n",
    "            R[state][random_action][observation] += reward\n",
    "\n",
    "            state = observation\n",
    "            counter += 1\n",
    "\n",
    "    # normalization\n",
    "    for i in range(T.shape[0]):\n",
    "        for j in range(T.shape[1]):\n",
    "            norm_coeff = np.sum(T[i, j, :])\n",
    "            if norm_coeff:\n",
    "                T[i, j, :] /= norm_coeff\n",
    "\n",
    "    counter_map[counter_map == 0] = 1  # avoid invalid division\n",
    "    R /= counter_map\n",
    "\n",
    "    return T, R\n",
    "\n",
    "\n",
    "# [link](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FORobd.png&f=1&nofb=1&ipt=9fb56f6e807783ea6559ee7c1bf9fd0c69a9b1a6b083f4e9aa326cb1dec195b2&ipo=images)\n",
    "def value_iteration(env, T, R, max_iter=1e5, test_iterations=1e3) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Value Iteration.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape [S, A] representing the state value function.\n",
    "    \"\"\"\n",
    "    num_state = T.shape[0]\n",
    "    num_action = T.shape[1]\n",
    "\n",
    "    success_rate = []\n",
    "\n",
    "    # discount factor\n",
    "    gamma = 0.9\n",
    "    # θ = 1e-04\n",
    "    theta = 1e-04\n",
    "\n",
    "    # value function\n",
    "    V = np.zeros(num_state)\n",
    "\n",
    "    # policy\n",
    "    P = np.zeros(num_state)\n",
    "\n",
    "    for _ in tqdm(range(int(max_iter)), desc=\"Value Iteration\"):\n",
    "        # for each state in the state space\n",
    "        for state_i in range(num_state):\n",
    "            action_value = 0\n",
    "            # update the value of the state\n",
    "            # V(s) ← max a∈A ∑ s′∈S P(s′|s,a)[R(s,a,s′) + γV(s′)]\n",
    "            action_values = []\n",
    "            for action_i in range(num_action):\n",
    "                tmp = 0\n",
    "                # for each transition in the transition space\n",
    "                for state_new in range(num_state):\n",
    "                    # transition P(s′|s,a)\n",
    "                    t = T[state_i][action_i][state_new]\n",
    "\n",
    "                    # reward R(s,a,s′)\n",
    "                    r = R[state_i][action_i][state_new]\n",
    "\n",
    "                    # new value V(s′)\n",
    "                    v_new = V[state_new]\n",
    "\n",
    "                    tmp += t * (r + gamma * v_new)\n",
    "                action_value = max(action_value, tmp)\n",
    "\n",
    "            # update the value of the state\n",
    "            V[state_i] = action_value\n",
    "\n",
    "            # update the policy\n",
    "            action_values = []\n",
    "            for state_i in range(num_state):\n",
    "                action_value = 0\n",
    "                for action_i in range(num_action):\n",
    "                    temp_value = 0\n",
    "                    for state_new in range(num_state):\n",
    "                        t = T[state_i][action_i][state_new]\n",
    "                        r = R[state_i][action_i][state_new]\n",
    "                        v_new = V[state_new]\n",
    "\n",
    "                        temp_value += t * (r + gamma * v_new)\n",
    "                    if temp_value > action_value:\n",
    "                        P[state_i] = action_i\n",
    "                        action_value = temp_value\n",
    "\n",
    "    # π(s) = argmax_a ∑(p(s', r|s, a)[r + γV(s')])\n",
    "    env.reset()\n",
    "    success = 0\n",
    "    for _ in tqdm(range(int(test_iterations)), desc=\"Testing policy\"):\n",
    "        state = env.reset()\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            action = P[state]\n",
    "            observation, reward, terminated, info = env.step(action)\n",
    "            if terminated:\n",
    "                if reward == 1:\n",
    "                    success += 1\n",
    "                break\n",
    "\n",
    "    success_rate.append(success / test_iterations)\n",
    "\n",
    "    return P, np.asarray(success_rate)\n",
    "\n",
    "\n",
    "def run_value_iteration(name, max_iter, test_iterations, test_samples):\n",
    "    # create directory for saving the model\n",
    "    current_ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    dir_name = f\"artifacts/{current_ts}/{name}/value_iteration/\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    env = gym.make(name)\n",
    "    env.reset()\n",
    "\n",
    "    # get the transition function\n",
    "    T, R = get_transitional_func(env, samples=test_samples)\n",
    "\n",
    "    # get the optimal policy\n",
    "    P, success_rate = value_iteration(env, T, R, max_iter=max_iter, test_iterations=test_iterations)\n",
    "\n",
    "    # save all artifacts\n",
    "    np.save(f\"{dir_name}/value_iteration/policy.npy\", P)\n",
    "    np.save(f\"{dir_name}/value_iteration/success_rate.npy\", success_rate)\n",
    "    np.save(f\"{dir_name}/value_iteration/transition.npy\", T)\n",
    "    np.save(f\"{dir_name}/value_iteration/reward.npy\", R)\n",
    "    # save hyperparameters\n",
    "    with open(f\"{dir_name}/value_iteration/hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"max_iter\": max_iter,\n",
    "                \"test_iterations\": test_iterations,\n",
    "                \"test_samples\": test_samples\n",
    "            }, f)\n",
    "\n",
    "    print(\"Final success rate of Value Iteration: {:.1f}%\".format(success_rate[-1] * 100))\n",
    "\n",
    "\n",
    "\n",
    "max_iter = 20\n",
    "test_iterations = 5\n",
    "test_samples = 1e3\n",
    "run_value_iteration('FrozenLake8x8-v1', max_iter, test_iterations, test_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [23], line 118\u001B[0m\n\u001B[0;32m    116\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[0;32m    117\u001B[0m startTime \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 118\u001B[0m optimal_policy \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iterations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m scores \u001B[38;5;241m=\u001B[39m evaluate_policy(env, optimalPolicy, gamma\u001B[38;5;241m=\u001B[39mgamma)\n\u001B[0;32m    120\u001B[0m endTime \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "Cell \u001B[1;32mIn [23], line 64\u001B[0m, in \u001B[0;36mpolicy_iteration\u001B[1;34m(env, gamma, max_iterations, epsilon)\u001B[0m\n\u001B[0;32m     61\u001B[0m record \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     62\u001B[0m record\u001B[38;5;241m.\u001B[39mappend(value_function)\n\u001B[1;32m---> 64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mrange\u001B[39;49m(max_iterations):\n\u001B[0;32m     65\u001B[0m     policy_stable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# policy evaluation\u001B[39;00m\n",
      "Cell \u001B[1;32mIn [23], line 64\u001B[0m, in \u001B[0;36mpolicy_iteration\u001B[1;34m(env, gamma, max_iterations, epsilon)\u001B[0m\n\u001B[0;32m     61\u001B[0m record \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     62\u001B[0m record\u001B[38;5;241m.\u001B[39mappend(value_function)\n\u001B[1;32m---> 64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mrange\u001B[39;49m(max_iterations):\n\u001B[0;32m     65\u001B[0m     policy_stable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# policy evaluation\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.2\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.2\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def execute(env, policy, gamma=1.0, render=False):\n",
    "    state = env.reset()\n",
    "    totalReward = 0\n",
    "    stepIndex = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        state_new, reward, terminated, info = env.step(int(policy[state]))\n",
    "        totalReward += (gamma ** stepIndex * reward)\n",
    "        stepIndex += 1\n",
    "        if terminated:\n",
    "            break\n",
    "    return totalReward\n",
    "\n",
    "#Evaluation\n",
    "def evaluate_policy(env, policy, gamma=1.0, n=100):\n",
    "    scores = []\n",
    "    for _ in range(n):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        index = 0\n",
    "        while True:\n",
    "            state_new, reward, terminated, info = env.step(int(policy[state]))\n",
    "            total_reward += (gamma ** index * reward)\n",
    "            index += 1\n",
    "            if terminated:\n",
    "                break\n",
    "        scores.append(total_reward)\n",
    "    scores = [execute(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "#Extract the policy given a value-function\n",
    "def extract_policy(v, gamma=1.0):\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
    "            policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "#Iteratively calculates value-function under policy\n",
    "def get_policy_value(env, policy, gamma=1.0, epsilon=0.1):\n",
    "    value = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        V = np.copy(value)\n",
    "        for states in range(env.observation_space.n):\n",
    "            policy_a = policy[states]\n",
    "            value[states] = sum([p * (r + gamma * V[s_]) for p,s_, r, _ in env.env.P[states][policy_a]])\n",
    "        if np.sum((np.fabs(V - value))) <= epsilon:\n",
    "            break\n",
    "    return value\n",
    "\n",
    "\n",
    "#Policy Iteration algorithm\n",
    "def policy_iteration(env, gamma=1.0, max_iterations=1000, epsilon=0.1):\n",
    "    # policy = np.random.choice(env.action_space.n, size=env.observation_space.n)\n",
    "    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
    "    value_function = np.zeros(env.observation_space.n)\n",
    "    new_policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    record = []\n",
    "    record.append(value_function)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        policy_stable = True\n",
    "\n",
    "        # policy evaluation\n",
    "        p_e_value_function = np.zeros(env.observation_space.n)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.observation_space.n):\n",
    "                v = p_e_value_function[s]\n",
    "                p_temp = env.P[s][policy[s]]\n",
    "                new_value = 0\n",
    "                for k in p_temp:\n",
    "                    new_value += k[0] * (k[2] + gamma * p_e_value_function[k[1]])\n",
    "                p_e_value_function[s] = new_value\n",
    "                delta = max(delta, abs(v - p_e_value_function[s]))\n",
    "            if delta <= epsilon:\n",
    "                break\n",
    "\n",
    "        value_function = p_e_value_function\n",
    "        record.append(value_function)\n",
    "\n",
    "        policy = new_policy.copy()\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.observation_space.n):\n",
    "                v = p_e_value_function[s]\n",
    "                p_temp = env.P[s][policy[s]]\n",
    "                new_value = 0\n",
    "                for k in p_temp:\n",
    "                    new_value += k[0] * (k[2] + gamma * p_e_value_function[k[1]])\n",
    "                p_e_value_function[s] = new_value\n",
    "                delta = max(delta, abs(v - p_e_value_function[s]))\n",
    "            if delta <= epsilon:\n",
    "                break\n",
    "\n",
    "        # policy improvement\n",
    "        new_policy\n",
    "\n",
    "        #\n",
    "\n",
    "        V = get_policy_value(env, policy, gamma, epsilon)\n",
    "        V_next = extract_policy(V, gamma)\n",
    "        if np.all(policy == V_next):\n",
    "            print('Policy Iteration converged at %d' %(i+1))\n",
    "            break\n",
    "        policy = V_next\n",
    "    return policy\n",
    "\n",
    "## Policy search\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "gamma = 0.9\n",
    "max_iterations = 100000\n",
    "epsilon = 0.001\n",
    "startTime = time.time()\n",
    "optimal_policy = policy_iteration(env, gamma = gamma, max_iterations=max_iterations, epsilon=epsilon)\n",
    "scores = evaluate_policy(env, optimalPolicy, gamma=gamma)\n",
    "endTime = time.time()\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(np.max(scores), endTime-startTime))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "DeprecatedEnv",
     "evalue": "Env FrozenLake-v0 not found (valid versions include ['FrozenLake-v1'])",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\gym\\envs\\registration.py:158\u001B[0m, in \u001B[0;36mEnvRegistry.spec\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv_specs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# Parse the env name and check to see if it matches the non-version\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# part of a valid env (could also check the exact number here)\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'FrozenLake-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mDeprecatedEnv\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [24], line 94\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m policy, success_rate\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 94\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFrozenLake-v0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m     env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;66;03m# toy policy\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\gym\\envs\\registration.py:235\u001B[0m, in \u001B[0;36mmake\u001B[1;34m(id, **kwargs)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake\u001B[39m(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m registry\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\gym\\envs\\registration.py:128\u001B[0m, in \u001B[0;36mEnvRegistry.make\u001B[1;34m(self, path, **kwargs)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    127\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaking new env: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, path)\n\u001B[1;32m--> 128\u001B[0m spec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m env \u001B[38;5;241m=\u001B[39m spec\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m env\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\gym\\envs\\registration.py:185\u001B[0m, in \u001B[0;36mEnvRegistry.spec\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    176\u001B[0m toytext_envs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKellyCoinflip\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKellyCoinflipGeneralized\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHotterColder\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    183\u001B[0m ]\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m matching_envs:\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mDeprecatedEnv(\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnv \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not found (valid versions include \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    187\u001B[0m             \u001B[38;5;28mid\u001B[39m, matching_envs\n\u001B[0;32m    188\u001B[0m         )\n\u001B[0;32m    189\u001B[0m     )\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m env_name \u001B[38;5;129;01min\u001B[39;00m algorithmic_envs:\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mUnregisteredEnv(\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlgorithmic environment \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m has been moved out of Gym. Install it via `pip install gym-algorithmic` and add `import gym_algorithmic` before using it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    193\u001B[0m             \u001B[38;5;28mid\u001B[39m\n\u001B[0;32m    194\u001B[0m         )\n\u001B[0;32m    195\u001B[0m     )\n",
      "\u001B[1;31mDeprecatedEnv\u001B[0m: Env FrozenLake-v0 not found (valid versions include ['FrozenLake-v1'])"
     ]
    }
   ],
   "source": [
    "def policyEval(policy, value, trans_prob, reward, gamma, max_itr=20):\n",
    "    \"\"\"\n",
    "    Policy evaluation\n",
    "    : param policy: ndarray, given policy\n",
    "    : param value: ndarray, given value function\n",
    "    : param trans_prob: ndarray, transition probabilities p(s'|a, s)\n",
    "    : param reward: ndarray, reward function r(s, a, s')\n",
    "    : param gamma: float, discount factor\n",
    "    : param max_itr: int, maximum number of iteration\n",
    "    : return: updated value function\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    num_state = policy.shape[0]\n",
    "\n",
    "    while counter < max_itr:\n",
    "        counter += 1\n",
    "        for s in range(num_state):\n",
    "            val = 0\n",
    "            for s_new in range(num_state):\n",
    "                val += trans_prob[s][policy[s]][s_new] * (\n",
    "                        reward[s][policy[s]][s_new] + gamma * value[s_new]\n",
    "                )\n",
    "            value[s] = val\n",
    "    return value\n",
    "\n",
    "\n",
    "def policyImprove(policy, value, trans_prob, reward, gamma):\n",
    "    \"\"\"\n",
    "    Policy improvement\n",
    "    : param policy: ndarray, given policy\n",
    "    : param value: ndarray, given value function\n",
    "    : param trans_prob: ndarray, transition probabilities p(s'|a, s)\n",
    "    : param reward: ndarray, reward function r(s, a, s')\n",
    "    : param gamma: float, discount factor\n",
    "    : return:\n",
    "        policy: updated policy\n",
    "        policy_stable, bool, True if no change in policy\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "    num_state = trans_prob.shape[0]\n",
    "    num_action = trans_prob.shape[1]\n",
    "\n",
    "    for s in range(num_state):\n",
    "        old_action = policy[s]\n",
    "        val = value[s]\n",
    "        for a in range(num_action):\n",
    "            tmp = 0\n",
    "            for s_new in range(num_state):\n",
    "                tmp += trans_prob[s][a][s_new] * (\n",
    "                        reward[s][a][s_new] + gamma * value[s_new]\n",
    "                )\n",
    "            if tmp > val:\n",
    "                policy[s] = a\n",
    "                val = tmp\n",
    "        if policy[s] != old_action:\n",
    "            policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "\n",
    "def policyItr(trans_prob, reward, gamma=0.99, max_itr=30, stop_if_stable=False):\n",
    "    \"\"\"\n",
    "    Policy iteration\n",
    "    : param trans_prob: ndarray, transition probabilities p(s'|a, s)\n",
    "    : param reward: ndarray, reward function r(s, a, s')\n",
    "    : param gamma: float, discount factor\n",
    "    : param max_itr: int, maximum number of iteration\n",
    "    : param stop_if_stable: bool, stop the training if reach stable state\n",
    "    : return:\n",
    "        policy: updated policy\n",
    "        success_rate: list, success rate for each iteration\n",
    "    \"\"\"\n",
    "    success_rate = []\n",
    "    num_state = trans_prob.shape[0]\n",
    "\n",
    "    # init policy and value function\n",
    "    policy = np.zeros(num_state, dtype=int)\n",
    "    value = np.zeros(num_state)\n",
    "\n",
    "    counter = 0\n",
    "    while counter < max_itr:\n",
    "        counter += 1\n",
    "        value = policyEval(policy, value, trans_prob, reward, gamma)\n",
    "        policy, stable = policyImprove(policy, value, trans_prob, reward, gamma)\n",
    "\n",
    "        # test the policy for each iteration\n",
    "        success_rate.append(testPolicy(policy))\n",
    "\n",
    "        if stable and stop_if_stable:\n",
    "            print(\"policy is stable at {} iteration\".format(counter))\n",
    "    return policy, success_rate\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"FrozenLake-v0\")\n",
    "    env.reset()\n",
    "\n",
    "    # toy policy\n",
    "    policy = [(s + 1) % 4 for s in range(15)]\n",
    "    print(\"Success rate of toy policy: {:.1f}%\".format(testPolicy(policy) * 100))\n",
    "\n",
    "    # get transitional probability and reward function\n",
    "    trans_prob, reward = learnModel(env)\n",
    "\n",
    "    # Policy Iteration\n",
    "    PI_policy, PI_success_rate = policyItr(trans_prob, reward, max_itr=50)\n",
    "    print(\"Final success rate of PI: {:.1f}%\".format(PI_success_rate[-1] * 100))\n",
    "\n",
    "    # plot\n",
    "    plot(PI_success_rate, \"Average success rate v.s. Episode (Policy Iteration)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [68]\u001B[0m, in \u001B[0;36m<cell line: 113>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    111\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m## Policy search\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m state_space_linear \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(itertools\u001B[38;5;241m.\u001B[39mproduct(\u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mrange\u001B[39m(s\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspaces\u001B[49m]))\n\u001B[0;32m    114\u001B[0m startTime \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    115\u001B[0m optimalPolicy \u001B[38;5;241m=\u001B[39m policyIteration(env, gamma \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Discrete' object has no attribute 'spaces'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Args:\n",
    "    poicy [S,A] shaped matrix representing policy.\n",
    "    env. OpenAi gym env.v.\n",
    "      env.P represents the transition propablities of the env\n",
    "      env.P[s][a] is a list of transition tuples\n",
    "      env.nS = is a number of states\n",
    "      env.nA is a number of actions\n",
    "    gamma: discount factor\n",
    "    render: boolean to turn rendering on/off\n",
    "\"\"\"\n",
    "\n",
    "def execute(env, policy, gamma=1.0, render=False):\n",
    "    start = env.reset()\n",
    "    totalReward = 0\n",
    "    stepIndex = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        # action = start[0]\n",
    "        if type(start) == type(()):\n",
    "            action = start[0]\n",
    "        else:\n",
    "            action = start\n",
    "        start, reward, done, truncated, info = env.step(int(policy[action]))\n",
    "        totalReward += (gamma ** stepIndex * reward)\n",
    "        stepIndex += 1\n",
    "        if done:\n",
    "            break\n",
    "    return totalReward\n",
    "\n",
    "#Evaluation\n",
    "def evaluatePolicy(env, policy, gamma=1.0, n=100):\n",
    "    scores = [execute(env, policy, gamma, False) for _ in range(n)]\n",
    "    return numpy.mean(scores)\n",
    "\n",
    "#Extract the policy given a value-function\n",
    "def extractPolicy(v, gamma=1.0):\n",
    "    policy = numpy.zeros([s.n for s in env.observation_space.spaces])\n",
    "    state_shape = [s.n for s in env.observation_space.spaces]\n",
    "    state_iterations = list(itertools.product(*[range(s.n) for s in env.observation_space.spaces]))\n",
    "    for s in state_iterations:\n",
    "        env_copy = deepcopy(env)\n",
    "        q_sa = numpy.zeros(state_shape)\n",
    "        for a in range(env.action_space.n):\n",
    "            env_copy = deepcopy(env_copy)\n",
    "            state_new, reward, terminated, truncated, info = env_copy.step(a)\n",
    "            for a2 in range(env.action_space.n):\n",
    "                s2, r2, t2, tt2, i2 = env_copy.step(a2)\n",
    "                q_sa[a] += 1.0 * (r2 + gamma * v[s2])\n",
    "        policy[s] = numpy.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "#Iteratively calculates value-function under policy\n",
    "def CalcPolicyValue(env, policy, gamma=1.0):\n",
    "    state_shape = [s.n for s in env.observation_space.spaces]\n",
    "    value = numpy.zeros(state_shape)\n",
    "    eps = 0.1\n",
    "    while True:\n",
    "        previousValue = numpy.copy(value)\n",
    "        state_iterations = list(itertools.product(*[range(s.n) for s in env.observation_space.spaces]))\n",
    "        for states in state_iterations:\n",
    "            policy_a = policy[states]\n",
    "            total_val = 0\n",
    "            env_copy = deepcopy(env)\n",
    "            state_new, reward, terminated, truncated, info = env_copy.step(policy_a)\n",
    "            i, j, k = state_new\n",
    "            k = int(k)\n",
    "            if terminated:\n",
    "                continue\n",
    "            for action in range(env.action_space.n):\n",
    "                s2, r2, t2, tt2, i2 = env_copy.step(action)\n",
    "                total_val += 1.0 * (r2 + gamma * previousValue[(i,j,k)])\n",
    "            value[states] = total_val\n",
    "        if (numpy.sum((numpy.fabs(previousValue - value))) <= eps):\n",
    "            break\n",
    "    return value\n",
    "\n",
    "\n",
    "#Policy Iteration algorithm\n",
    "def policyIteration(env, gamma=1.0):\n",
    "    policy = numpy.random.choice(env.action_space.n, size=(len(state_space_linear)))\n",
    "    maxIterations = 1000\n",
    "    gamma = 1.0\n",
    "    for i in range(maxIterations):\n",
    "        oldPolicyValue = CalcPolicyValue(env, policy, gamma)\n",
    "        newPolicy = extractPolicy(oldPolicyValue, gamma)\n",
    "        if (numpy.all(policy == newPolicy)):\n",
    "            print('Policy Iteration converged at %d' %(i+1))\n",
    "            break\n",
    "        policy = newPolicy\n",
    "    return policy\n",
    "\n",
    "def state_idx(state):\n",
    "    try:\n",
    "        if type(state) == type(()) and len(state) == 3:\n",
    "            return state_space_linear.index((state[0], state[1], int(state[2])))\n",
    "        if type(state) == type(()) and len(state) == 2:\n",
    "            state = state[0]\n",
    "            return state_space_linear.index((state[0], state[1], int(state[2])))\n",
    "        if type(state) == type(1):\n",
    "            return state\n",
    "    except:\n",
    "        return Exception('ugh oh')\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.reset()\n",
    "## Policy search\n",
    "state_space_linear = list(itertools.product(*[range(s.n) for s in env.observation_space.spaces]))\n",
    "startTime = time.time()\n",
    "optimalPolicy = policyIteration(env, gamma = 1.0)\n",
    "scores = evaluatePolicy(env, optimalPolicy, gamma=1.0)\n",
    "endTime = time.time()\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(numpy.max(scores), endTime-startTime))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [69]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m learning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.025\u001B[39m\n\u001B[0;32m      8\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[1;32m----> 9\u001B[0m state_space_linear \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(itertools\u001B[38;5;241m.\u001B[39mproduct(\u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mrange\u001B[39m(s\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspaces\u001B[49m]))\n\u001B[0;32m     10\u001B[0m Q \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros([\u001B[38;5;28mlen\u001B[39m(state_space_linear), env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn])\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstate_idx\u001B[39m(state):\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Discrete' object has no attribute 'spaces'"
     ]
    }
   ],
   "source": [
    "# deep q learning\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "state = env.reset()\n",
    "\n",
    "discount = 0.9\n",
    "learning_rate = 0.025\n",
    "epsilon = 0.5\n",
    "state_space_linear = list(itertools.product(*[range(s.n) for s in env.observation_space.spaces]))\n",
    "Q = np.zeros([len(state_space_linear), env.action_space.n])\n",
    "\n",
    "def state_idx(state):\n",
    "    try:\n",
    "        if type(state) == type(()) and len(state) == 3:\n",
    "            return state_space_linear.index((state[0], state[1], int(state[2])))\n",
    "        if type(state) == type(()) and len(state) == 2:\n",
    "            state = state[0]\n",
    "            return state_space_linear.index((state[0], state[1], int(state[2])))\n",
    "        if type(state) == type(1):\n",
    "            return state\n",
    "    except:\n",
    "        return Exception('ugh oh')\n",
    "\n",
    "\n",
    "for _ in range(50000):\n",
    "    state = state_idx(state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = state_idx(next_state)\n",
    "\n",
    "    # Q-Learning update:\n",
    "    # Q(s,a) <-- Q(s,a) + α * (r + γ max_a' Q(s',a') - Q(s,a))\n",
    "    target = reward - Q[state, action]\n",
    "    if not done:\n",
    "        target += discount * np.max(Q[next_state])\n",
    "    Q[state, action] += learning_rate * target\n",
    "\n",
    "    # Reset the environment if we're done\n",
    "    state = env.reset() if done else next_state\n",
    "\n",
    "# Now let's see what the value function looks like after training:\n",
    "V = np.max(Q, axis=1)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_idx(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 2, False), {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}