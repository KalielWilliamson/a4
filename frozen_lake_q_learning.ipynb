{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import gym\n",
    "import tqdm\n",
    "import itertools\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Iterations:  35%|███▍      | 28/81 [00:02<00:05,  9.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [21], line 39\u001B[0m\n\u001B[0;32m     37\u001B[0m max_steps, max_iteration, gamma, learning_rate \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m     38\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mprocess_time()\n\u001B[1;32m---> 39\u001B[0m reward_list, Q \u001B[38;5;241m=\u001B[39m \u001B[43mq_learn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmax_iteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m wall_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mprocess_time() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[0;32m     41\u001B[0m results \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_iteration\u001B[39m\u001B[38;5;124m'\u001B[39m: max_iteration,\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_steps\u001B[39m\u001B[38;5;124m'\u001B[39m: max_steps,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQ\u001B[39m\u001B[38;5;124m'\u001B[39m: Q\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m     49\u001B[0m }\n",
      "Cell \u001B[1;32mIn [21], line 17\u001B[0m, in \u001B[0;36mq_learn\u001B[1;34m(max_iteration, max_steps, learning_rate, gamma)\u001B[0m\n\u001B[0;32m     15\u001B[0m action_list \u001B[38;5;241m=\u001B[39m Q[state,:]\n\u001B[0;32m     16\u001B[0m fuzzy_actions \u001B[38;5;241m=\u001B[39m action_list \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m1\u001B[39m,env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn)\u001B[38;5;241m*\u001B[39m(\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m/\u001B[39m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 17\u001B[0m selected_action \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfuzzy_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m new_state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(selected_action)\n\u001B[0;32m     19\u001B[0m Q[state,selected_action] \u001B[38;5;241m=\u001B[39m Q[state,selected_action] \u001B[38;5;241m+\u001B[39m learning_rate\u001B[38;5;241m*\u001B[39m(reward \u001B[38;5;241m+\u001B[39m gamma \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(Q[new_state,:]) \u001B[38;5;241m-\u001B[39m Q[state,selected_action])\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(a, axis, out, keepdims)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1130\u001B[0m \u001B[38;5;124;03mReturns the indices of the maximum values along an axis.\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;124;03m(2, 1, 4)\u001B[39;00m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m kwds \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeepdims\u001B[39m\u001B[38;5;124m'\u001B[39m: keepdims} \u001B[38;5;28;01mif\u001B[39;00m keepdims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39m_NoValue \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margmax\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32mD:\\conda\\MDP\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def low_pass(data, alpha=0.99):\n",
    "    low_pass = [data[0]]\n",
    "    for i in range(1,len(data)):\n",
    "        low_pass.append(alpha*low_pass[-1] + (1.0-alpha)*data[i] )\n",
    "    return low_pass\n",
    "\n",
    "\n",
    "def q_learn(max_iteration, max_steps, learning_rate, gamma):\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    reward_list = []\n",
    "    for i in range(max_iteration):\n",
    "        state = env.reset()\n",
    "        reward_sum = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            action_list = Q[state,:]\n",
    "            fuzzy_actions = action_list + np.random.randn(1,env.action_space.n)*(1. / (i + 1))\n",
    "            selected_action = np.argmax(fuzzy_actions)\n",
    "            new_state, reward, done, _ = env.step(selected_action)\n",
    "            Q[state,selected_action] = Q[state,selected_action] + learning_rate*(reward + gamma * np.max(Q[new_state,:]) - Q[state,selected_action])\n",
    "            state = new_state\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_list.append(reward_sum)\n",
    "    return reward_list, Q\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "\n",
    "max_steps = [1e2, 1e3, 1e4]\n",
    "max_iters = [1e2, 1e4, 1e6]\n",
    "gammas = [0.1, 0.6, 0.9]\n",
    "learning_rates = [0.1, 0.5, 0.9]\n",
    "hyperparameters = list(itertools.product(max_steps, max_iters, gammas, learning_rates))\n",
    "\n",
    "for args in tqdm.tqdm(hyperparameters, desc=\"Experiment Iterations\"):\n",
    "    env.reset()\n",
    "    max_steps, max_iteration, gamma, learning_rate = args\n",
    "    start_time = time.process_time()\n",
    "    reward_list, Q = q_learn(int(max_steps), int(max_iteration), learning_rate, gamma)\n",
    "    wall_time = time.process_time() - start_time\n",
    "    results = {\n",
    "        'max_iteration': max_iteration,\n",
    "        'max_steps': max_steps,\n",
    "        'gamma': gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "        'wall_time': wall_time,\n",
    "        'training_loss': reward_list,\n",
    "        'Q': Q.tolist()\n",
    "    }\n",
    "    with open(f'artifacts/frozen_lake_q_learning/{time.time_ns()}.json', 'w') as f:\n",
    "        json.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}